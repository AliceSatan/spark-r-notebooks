{"name":"SparkR notebooks","tagline":" R on Spark (SparkR) tutorials for Big Data analysis and Machine Learning as IPython / Jupyter notebooks","body":"# SparkR Notebooks  \r\n\r\nThis is a collection of [Jupyter](https://jupyter.org/) \r\nnotebooks intended to train the reader on different [Apache Spark](http://spark.apache.org/) concepts, from \r\nbasic to advanced, by using the **R** language.  \r\n\r\nIf your are interested in being introduced to some basic Data Science Engineering concepts and applications, you might find [these series of tutorials](https://github.com/jadianes/data-science-your-way) interesting. There we explain different concepts and applications \r\nusing Python and R. Additionally, if you are interested in using Python with Spark, you can have a look at our [pySpark notebooks]().    \r\n\r\n## Instructions  \r\n\r\nFor these series of notebooks, we have used [Jupyter](https://jupyter.org/) with the [IRkernel](http://irkernel.github.io/) R kernel. You can find installation instructions for you specific setup [here](http://irkernel.github.io/installation/). Have also a look at [Andrie de Vries](https://twitter.com/RevoAndrie) post [Using R with Jupyter Notebooks](http://blog.revolutionanalytics.com/2015/09/using-r-with-jupyter-notebooks.html) that includes instructions for installing Jupyter and IRkernel together.   \r\n\r\nA good way of using these notebooks is by first cloning the repo, and then \r\nstarting your [Jupyter](https://jupyter.org/) in **pySpark mode**. For example, \r\nif we have a *standalone* Spark installation running in our `localhost` with a \r\nmaximum of 6Gb per node assigned to IPython:  \r\n\r\n    MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook --pylab inline\" ~/spark-1.5.0-bin-hadoop2.6/bin/pyspark\r\n\r\nNotice that the path to the `pyspark` command will depend on your specific \r\ninstallation. So as requirement, you need to have\r\n[Spark installed](https://spark.apache.org/docs/latest/index.html) in \r\nthe same machine you are going to start the `IPython notebook` server.     \r\n\r\nFor more Spark options see [here](https://spark.apache.org/docs/latest/spark-standalone.html). In general it works the rule of passign options \r\ndescribed in the form `spark.executor.memory` as `SPARK_EXECUTOR_MEMORY` when\r\ncalling IPython/pySpark.   \r\n\r\n\r\n## Datasets  \r\n\r\n#### [2013 American Community Survey dataset](http://www.census.gov/programs-surveys/acs/data/summary-file.html)  \r\n\r\nEvery year, the US Census Bureau runs the American Community Survey. In this survey, approximately 3.5 million \r\nhouseholds are asked detailed questions about who they are and how they live. Many topics are covered, including \r\nancestry, education, work, transportation, internet use, and residency. You can directly to \r\n[the source](http://www.census.gov/programs-surveys/acs/data/summary-file.html) \r\nin order to know more about the data and get files for different years, longer periods, individual states, etc. \r\n\r\nIn any case, the [starting up notebook](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb0-starting-up/nb0-starting-up.ipynb) \r\nwill download the 2013 data locally for later use with the rest of the notebooks. \r\n\r\nThe idea of using this dataset came from being recently [announced in Kaggle](https://www.kaggle.com/c/2013-american-community-survey)\r\n as part of their Kaggle scripts datasets. There you will be able to analyse the dataset on site, while sharing your results with other Kaggle\r\nusers. Highly recommended!  \r\n\r\n## Notebooks  \r\n\r\n#### [Downloading data and starting with SparkR](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb0-starting-up/nb0-starting-up.ipynb)  \r\n\r\nWhere we download our data locally and start up a SparkR cluster.  \r\n\r\n#### [SparkSQL basics with SparkR](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb1-spark-sql-basics/nb1-spark-sql-basics.ipynb)  \r\n\r\nAbout loading our data into SparkSQL data frames using SparkR.  \r\n\r\n#### [Data frame operations with SparkSQL and SparkR](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb2-spark-sql-operations/nb2-spark-sql-operations.ipynb)  \r\n\r\nDifferent operations we can use with SparkR and `DataFrame` objects, such as data selection and filtering, aggregations, and sorting. The basis for exploratory data analysis and machine learning.  \r\n\r\n#### [Exploratory Data Analysis with SparkR and ggplot2](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb3-eda/nb3-eda.ipynb)  \r\n\r\nHow to explore different types of variables using SparkR and ggplot2 charts.  \r\n\r\n\r\n#### [Linear Models with SparkR](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb4-linear-models/nb4-linear-models.ipynb)  \r\n\r\nAbout linear models using SparkR, its uses and current limitations in v1.5.  \r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}