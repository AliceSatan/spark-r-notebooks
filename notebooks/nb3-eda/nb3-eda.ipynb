{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis with SparkR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Introduction to Apache Spark with R by J. A. Dianes**](https://github.com/jadianes/spark-r-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use all the SparkSQL operation we learned before in order to explore property value reltion with other variables in the [2013 American Community Survey](http://www.census.gov/programs-surveys/acs/data/summary-file.html) dataset. The whole point of R on Spark is to introduce Spark scalability into R data analysis pipelines. With this idea in mind, we have seen how [SparkR documentation](http://spark.apache.org/docs/latest/sparkr.html) introduces data types and functions that are very similar to what we are used to when using regular R. We will combine these with [ggplot2](http://ggplot2.org) in order to explore relationships between our varaibles. We will explain what we do at every step but, if you want to go deeper into `ggplot2` for exploratory data analysis, I did this [Udacity on-line course](https://www.udacity.com/course/data-analysis-with-r--ud651) in the past and I highly recommend it! \n",
    "\n",
    "So let's dive into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a SparkSQL context and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to explore our data, we first need to load it into a SparkSQL data frame. But first we need to init a SparkSQL context. The first thing we need to do is to set up some environment variables and library paths as follows. Remember to replace the value assigned to `SPARK_HOME` with your Spark home folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Spark home and R libs\n",
    "Sys.setenv(SPARK_HOME='/home/cluster/spark-1.5.0-bin-hadoop2.6')\n",
    ".libPaths(c(file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'), .libPaths()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the `SparkR` library as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘SparkR’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, na.omit\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, rbind, sample, subset, summary, table, transform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(SparkR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can initialise the Spark context as [in the official documentation](http://spark.apache.org/docs/latest/sparkr.html#starting-up-sparkcontext-sqlcontext). In our case we are use a standalone Spark cluster with one master and seven workers. If you are running Spark in local node, use just `master='local'`. Additionally, we require a Spark package from Databricks to read CSV files (more on this in the [previous notebook](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb1-spark-sql-basics/nb1-spark-sql-basics.ipynb)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching java with spark-submit command /home/cluster/spark-1.5.0-bin-hadoop2.6/bin/spark-submit  --packages com.databricks:spark-csv_2.11:1.2.0 sparkr-shell /tmp/RtmpEC6ItW/backend_port4bd622e97d86 \n"
     ]
    }
   ],
   "source": [
    "sc <- sparkR.init(master='spark://169.254.206.2:7077', sparkPackages=\"com.databricks:spark-csv_2.11:1.2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can start the SparkSQL context as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext <- sparkRSQL.init(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our SparkSQL context ready, we can use it to load our CSV data into data frames. We have downloaded our [2013 American Community Survey dataset](http://www.census.gov/programs-surveys/acs/data/summary-file.html) files in [notebook 0](https://github.com/jadianes/spark-r-notebooks/tree/master/notebooks/nb0-starting-up/nb0-starting-up.ipynb), so they should be stored locally. Remember to set the right path for your data files in the first line, ours is `/nfs/data/2013-acs/ss13husa.csv`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_file_path <- file.path('', 'nfs','data','2013-acs','ss13husa.csv')\n",
    "housing_b_file_path <- file.path('', 'nfs','data','2013-acs','ss13husb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read into a SparkSQL dataframe. We need to pass four parameters in addition to the `sqlContext`:  \n",
    "\n",
    "- The file path.  \n",
    "- `header='true'` since our `csv` files have a header with the column names. \n",
    "- Indicate that we want the library to infer the schema.  \n",
    "- And the source type (the Databricks package in this case). \n",
    "\n",
    "And we have two separate files for both, housing and population data. We need to join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_df <- read.df(sqlContext, \n",
    "                        housing_a_file_path, \n",
    "                        header='true', \n",
    "                        source = \"com.databricks.spark.csv\", \n",
    "                        inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_b_df <- read.df(sqlContext, \n",
    "                        housing_b_file_path, \n",
    "                        header='true', \n",
    "                        source = \"com.databricks.spark.csv\", \n",
    "                        inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_df <- rbind(housing_a_df, housing_b_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have everything there by counting the files and listing a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1476313"
      ],
      "text/latex": [
       "1476313"
      ],
      "text/markdown": [
       "1476313"
      ],
      "text/plain": [
       "[1] 1476313"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrow(housing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>RT</th><th scope=col>SERIALNO</th><th scope=col>DIVISION</th><th scope=col>PUMA</th><th scope=col>REGION</th><th scope=col>ST</th><th scope=col>ADJHSG</th><th scope=col>ADJINC</th><th scope=col>WGTP</th><th scope=col>NP</th><th scope=col>ellip.h</th><th scope=col>wgtp71</th><th scope=col>wgtp72</th><th scope=col>wgtp73</th><th scope=col>wgtp74</th><th scope=col>wgtp75</th><th scope=col>wgtp76</th><th scope=col>wgtp77</th><th scope=col>wgtp78</th><th scope=col>wgtp79</th><th scope=col>wgtp80</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>H</td><td>84</td><td>6</td><td>2600</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>0</td><td>1</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>H</td><td>154</td><td>6</td><td>2500</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>51</td><td>4</td><td>⋯</td><td>86</td><td>53</td><td>59</td><td>84</td><td>49</td><td>15</td><td>15</td><td>20</td><td>50</td><td>16</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>H</td><td>156</td><td>6</td><td>1700</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>449</td><td>1</td><td>⋯</td><td>161</td><td>530</td><td>601</td><td>579</td><td>341</td><td>378</td><td>387</td><td>421</td><td>621</td><td>486</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>H</td><td>160</td><td>6</td><td>2200</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>16</td><td>3</td><td>⋯</td><td>31</td><td>24</td><td>33</td><td>7</td><td>7</td><td>13</td><td>18</td><td>23</td><td>23</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>H</td><td>231</td><td>6</td><td>2400</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>52</td><td>1</td><td>⋯</td><td>21</td><td>18</td><td>37</td><td>49</td><td>103</td><td>38</td><td>49</td><td>51</td><td>46</td><td>47</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>H</td><td>286</td><td>6</td><td>900</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>76</td><td>1</td><td>⋯</td><td>128</td><td>25</td><td>68</td><td>66</td><td>80</td><td>26</td><td>66</td><td>164</td><td>88</td><td>24</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & RT & SERIALNO & DIVISION & PUMA & REGION & ST & ADJHSG & ADJINC & WGTP & NP & ellip.h & wgtp71 & wgtp72 & wgtp73 & wgtp74 & wgtp75 & wgtp76 & wgtp77 & wgtp78 & wgtp79 & wgtp80\\\\\n",
       "\\hline\n",
       "\t1 & H & 84 & 6 & 2600 & 3 & 1 & 1000000 & 1007549 & 0 & 1 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t2 & H & 154 & 6 & 2500 & 3 & 1 & 1000000 & 1007549 & 51 & 4 & ⋯ & 86 & 53 & 59 & 84 & 49 & 15 & 15 & 20 & 50 & 16\\\\\n",
       "\t3 & H & 156 & 6 & 1700 & 3 & 1 & 1000000 & 1007549 & 449 & 1 & ⋯ & 161 & 530 & 601 & 579 & 341 & 378 & 387 & 421 & 621 & 486\\\\\n",
       "\t4 & H & 160 & 6 & 2200 & 3 & 1 & 1000000 & 1007549 & 16 & 3 & ⋯ & 31 & 24 & 33 & 7 & 7 & 13 & 18 & 23 & 23 & 5\\\\\n",
       "\t5 & H & 231 & 6 & 2400 & 3 & 1 & 1000000 & 1007549 & 52 & 1 & ⋯ & 21 & 18 & 37 & 49 & 103 & 38 & 49 & 51 & 46 & 47\\\\\n",
       "\t6 & H & 286 & 6 & 900 & 3 & 1 & 1000000 & 1007549 & 76 & 1 & ⋯ & 128 & 25 & 68 & 66 & 80 & 26 & 66 & 164 & 88 & 24\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  RT SERIALNO DIVISION PUMA REGION ST  ADJHSG  ADJINC WGTP NP TYPE ACCESS ACR\n",
       "1  H       84        6 2600      3  1 1000000 1007549    0  1    3     NA  NA\n",
       "2  H      154        6 2500      3  1 1000000 1007549   51  4    1      1   1\n",
       "3  H      156        6 1700      3  1 1000000 1007549  449  1    1      3   1\n",
       "4  H      160        6 2200      3  1 1000000 1007549   16  3    1      3   3\n",
       "5  H      231        6 2400      3  1 1000000 1007549   52  1    1      3  NA\n",
       "6  H      286        6  900      3  1 1000000 1007549   76  1    1      1   1\n",
       "  AGS BATH BDSP BLD BROADBND BUS COMPOTHX CONP DIALUP DSL ELEP FIBEROP FS FULP\n",
       "1  NA   NA   NA  NA       NA  NA       NA   NA     NA  NA   NA      NA  2   NA\n",
       "2  NA    1    3   2        2   2        2   NA      2   2  350       1  2    2\n",
       "3  NA    1    3   2       NA   2        2   NA     NA  NA  300      NA  2    2\n",
       "4   1    1    4   2       NA   2        2   NA     NA  NA  220      NA  1    2\n",
       "5  NA    1    1   5       NA  NA        2   NA     NA  NA   60      NA  1    1\n",
       "6  NA    1    4   2        2   2        2   NA      2   2  100       2  2    2\n",
       "  GASP HANDHELD HFL INSP LAPTOP MHP MODEM MRGI MRGP MRGT MRGX OTHSVCEX REFR\n",
       "1   NA       NA  NA   NA     NA  NA    NA   NA   NA   NA   NA       NA   NA\n",
       "2    3        2   3  350      1  NA     2   NA   NA   NA    3        2    1\n",
       "3    3        2   3  980      1  NA    NA    1  550    2    1       NA    1\n",
       "4   20        1   3   NA      2  NA    NA   NA   NA   NA   NA       NA    1\n",
       "5    3        2   1   NA      2  NA    NA   NA   NA   NA   NA       NA    1\n",
       "6   90        2   1   50      2  NA     1    2  290    2    1        2    1\n",
       "  RMSP RNTM RNTP RWAT RWATPR SATELLITE SINK SMP STOV TEL TEN TOIL VACS  VALP\n",
       "1   NA   NA   NA   NA     NA        NA   NA  NA   NA  NA  NA   NA   NA    NA\n",
       "2    9   NA   NA    1      9         2    1  NA    1   1   2    1   NA 25000\n",
       "3    6   NA   NA    1      9        NA    1  NA    1   1   1    1   NA 80000\n",
       "4    6    2  100    1      9        NA    1  NA    1   1   3    1   NA    NA\n",
       "5    3    2   80    1      9        NA    1  NA    1   1   3    1   NA    NA\n",
       "6    9   NA   NA    1      9         2    1  NA    1   1   1    1   NA 18000\n",
       "  VEH WATP YBL FES FFINCP FGRNTP FHINCP  FINCP FPARC FSMOCP GRNTP GRPIP HHL HHT\n",
       "1  NA   NA  NA  NA     NA     NA     NA     NA    NA     NA    NA    NA  NA  NA\n",
       "2   3  480   2   1      1     NA      1 151000     4      0    NA    NA   1   1\n",
       "3   1  700   5  NA      0     NA      1     NA    NA      1    NA    NA   1   6\n",
       "4   2  360   6   8      1      0      1  11400     2     NA   370    39   1   3\n",
       "5   0    1   5  NA      0      0      0     NA    NA     NA   140    43   1   6\n",
       "6   1  370   2  NA      0     NA      1     NA    NA      1    NA    NA   1   6\n",
       "   HINCP HUGCL HUPAC HUPAOC HUPARC KIT LNGI MULTG MV NOC NPF NPP NR NRC OCPIP\n",
       "1     NA    NA    NA     NA     NA  NA   NA    NA NA  NA  NA  NA NA  NA    NA\n",
       "2 151000     0     4      4      4   1    1     1  6   0   4   0  0   0     3\n",
       "3  39930     0     4      4      4   1    1     1  7   0  NA   0  0   0    28\n",
       "4  11400     1     2      4      2   1    1     1  4   0   3   1  0   1    NA\n",
       "5   3900     0     4      4      4   1    1     1  6   0  NA   0  0   0    NA\n",
       "6   5400     0     4      4      4   1    1     1  7   0  NA   0  0   0   101\n",
       "  PARTNER PLM PSF R18 R60 R65 RESMODE SMOCP SMX SRNT SSMC SVAL TAXP WIF WKEXREL\n",
       "1      NA  NA  NA  NA  NA  NA      NA    NA  NA   NA   NA   NA   NA  NA      NA\n",
       "2       0   1   0   0   0   0       2   426  NA    0    0    1    3   2       1\n",
       "3       0   1   0   0   1   0       2   926   3    0    0    1    6  NA      NA\n",
       "4       0   1   0   1   1   0       2    NA  NA    0    0    0   NA   1      15\n",
       "5       0   1   0   0   1   1       1    NA  NA    1    0    0   NA  NA      NA\n",
       "6       0   1   0   0   1   1       1   522   3    0    0    1    3  NA      NA\n",
       "  WORKSTAT FACCESSP FACRP FAGSP FBATHP FBDSP FBLDP FBROADBNDP FBUSP FCOMPOTHXP\n",
       "1       NA       NA    NA    NA     NA    NA    NA         NA    NA         NA\n",
       "2        1        0     0     0      0     0     0          0     0          0\n",
       "3       NA        0     0     0      0     0     0          0     0          0\n",
       "4       15        0     1     1      0     0     0          0     0          0\n",
       "5       NA        0     0     0      0     0     0          0     0          0\n",
       "6       NA        0     1     0      0     0     1          1     1          1\n",
       "  FCONP FDIALUPP FDSLP FELEP FFIBEROPP FFSP FFULP FGASP FHANDHELDP FHFLP FINSP\n",
       "1    NA       NA    NA    NA        NA    0    NA    NA         NA    NA    NA\n",
       "2     0        0     0     0         0    0     0     0          0     0     1\n",
       "3     0        0     0     0         0    0     0     0          0     0     1\n",
       "4     0        0     0     0         0    0     0     0          0     0     0\n",
       "5     0        0     0     0         0    0     0     0          0     0     0\n",
       "6     0        1     1     1         1    0     1     1          1     0     1\n",
       "  FKITP FLAPTOPP FMHP FMODEMP FMRGIP FMRGP FMRGTP FMRGXP FMVP FOTHSVCEXP FPLMP\n",
       "1    NA       NA   NA      NA     NA    NA     NA     NA    0         NA    NA\n",
       "2     0        0    0       0      0     0      0      0    0          0     0\n",
       "3     0        0    0       0      1     1      1      1    1          0     0\n",
       "4     0        0    0       0      0     0      0      0    0          0     0\n",
       "5     0        0    0       0      0     0      0      0    0          0     0\n",
       "6     0        1    0       1      1     1      1      1    0          1     0\n",
       "  FREFRP FRMSP FRNTMP FRNTP FRWATP FRWATPRP FSATELLITEP FSINKP FSMP FSMXHP\n",
       "1     NA    NA     NA    NA     NA       NA          NA     NA   NA     NA\n",
       "2      0     0      0     0      0        0           0      0    0      0\n",
       "3      0     0      0     0      0        0           0      0    1      1\n",
       "4      0     0      0     0      0        0           0      0    0      0\n",
       "5      0     0      0     0      0        0           0      0    0      0\n",
       "6      0     0      0     0      0        0           1      0    1      1\n",
       "  FSMXSP FSTOVP FTAXP FTELP FTENP FTOILP FVACSP FVALP FVEHP FWATP FYBLP wgtp1\n",
       "1     NA     NA    NA    NA    NA     NA     NA    NA    NA    NA    NA     0\n",
       "2      0      0     1     0     0      0      0     0     0     0     0    45\n",
       "3      1      0     1     0     1      0      0     1     0     0     0   481\n",
       "4      0      0     0     0     0      0      0     0     0     0     0    21\n",
       "5      0      0     0     0     0      0      0     0     0     0     0    74\n",
       "6      1      0     1     0     1      0      0     1     0     1     0    84\n",
       "  wgtp2 wgtp3 wgtp4 wgtp5 wgtp6 wgtp7 wgtp8 wgtp9 wgtp10 wgtp11 wgtp12 wgtp13\n",
       "1     0     0     0     0     0     0     0     0      0      0      0      0\n",
       "2    52    53    50   100    79    78    50    19     57     89     46     67\n",
       "3   576   807   739   154   173   722   455   157    671    681    470    403\n",
       "4    14     6    20    30    16     5    25     4     27     11     28     14\n",
       "5    50    43    13    12   102    80    20    60     98     17     18     52\n",
       "6    69    19   118    91    77    20    76    73    180     68     78     24\n",
       "  wgtp14 wgtp15 wgtp16 wgtp17 wgtp18 wgtp19 wgtp20 wgtp21 wgtp22 wgtp23 wgtp24\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2    109     51     18     17     18     47     87     49     50     49     64\n",
       "3    118    339    436    443    393    160    411    468    387    127    150\n",
       "4     15     27     40     31      4     16     20      9     26     39     32\n",
       "5     45     95     57     46     75     47     54     16     43     57    122\n",
       "6    131    136     73    143     82     26     67     21     27     63     74\n",
       "  wgtp25 wgtp26 wgtp27 wgtp28 wgtp29 wgtp30 wgtp31 wgtp32 wgtp33 wgtp34 wgtp35\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     13     16     16     55     86     48     14     49     50     11     56\n",
       "3    553    514    133    613    562    151    196    432    577    710    378\n",
       "4     38      9     15      6     19     22      5     16      3     27     21\n",
       "5     93     17     13     61     59     23     82     69     50     55     20\n",
       "6     24    119     87    126    131     67    130     30     79     81     70\n",
       "  wgtp36 wgtp37 wgtp38 wgtp39 wgtp40 wgtp41 wgtp42 wgtp43 wgtp44 wgtp45 wgtp46\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     83     71    108     54     81     43     53     50     63     17     15\n",
       "3    481    504    531    760    409    560    404    637    516    101    150\n",
       "4     33     14     11      6     31     10     21     22     11      8     22\n",
       "5     59     62     44     38     40     16     47     40    107     88     16\n",
       "6     19     94    119     65     23     84     83     23    111     70     73\n",
       "  wgtp47 wgtp48 wgtp49 wgtp50 wgtp51 wgtp52 wgtp53 wgtp54 wgtp55 wgtp56 wgtp57\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     15     47     88     47     14     52     51     20     53     84     78\n",
       "3    659    454    179    793    724    451    527    168    355    445    465\n",
       "4     27     16     30      6     28      9     17     12     23      3      4\n",
       "5     14     86     59     16     92    113     61     64     14     74     48\n",
       "6     22     86     69    123     91     75     28    138     98     69    121\n",
       "  wgtp58 wgtp59 wgtp60 wgtp61 wgtp62 wgtp63 wgtp64 wgtp65 wgtp66 wgtp67 wgtp68\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     74     53     15     54     55     58     65    110     82     89     50\n",
       "3    426    228    575    386    433    167    230    797    572    155    556\n",
       "4     38     15     17     33      8     10      7      7     21     22     19\n",
       "5     43     53     50     83     47     51     15     16     93     95     15\n",
       "6     87     23     67     18     24     77     62     28    125     72    121\n",
       "  wgtp69 wgtp70 wgtp71 wgtp72 wgtp73 wgtp74 wgtp75 wgtp76 wgtp77 wgtp78 wgtp79\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     12     55     86     53     59     84     49     15     15     20     50\n",
       "3    596    170    161    530    601    579    341    378    387    421    621\n",
       "4     12     11     31     24     33      7      7     13     18     23     23\n",
       "5     47    106     21     18     37     49    103     38     49     51     46\n",
       "6    135    100    128     25     68     66     80     26     66    164     88\n",
       "  wgtp80\n",
       "1      0\n",
       "2     16\n",
       "3    486\n",
       "4      5\n",
       "5     47\n",
       "6     24"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(housing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring property value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our [data dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt) we know that the variable `VALP` represents a property value. Our goal in this notebook is to explore this variable, specially what other variables might have certain relationship with it. \n",
    "\n",
    "A possible way of doing this is to calculate correlation values between `VALP` and any other variable in the 230 set of them. For this we need numeric variables, and most of our dataset features are not numeric. There are other ways of doing this for categorical values but in our case we will use a more informal approach.  \n",
    "\n",
    "First we will have a look at the dictionary and extract a list of candidates that to our knowledge could influence a propery value. Then we will use aggregations, tables, and plots, to explore relationships. The process won't be exhaustive but will show how to do this kind of analysis. Actually we won't to make this notebook so long, so we wil be fine with a reduced list of candidates. Hopefully we will get a list of variables to be used in a predictive linear model in further notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 16 variables we will consider:  \n",
    "\n",
    "- `ST`: state code, as the main geographical variable, since it someway includes region and department.  \n",
    "- `ACR`: Lot size:           \n",
    "  - b:  N/A (GQ/not a one-family house or mobile home)\n",
    "  - 1:  House on less than one acre\n",
    "  - 2:  House on one to less than ten acres \n",
    "  - 3:  House on ten or more acres\n",
    "- `BLD`:\n",
    "- `RMSP`: \n",
    "- `VACS`:\n",
    "- `YBL`:\n",
    "- A series of facilities variables:\n",
    "  - `ACCESS`: Access to the Internet\n",
    "           b .N/A (GQ)\n",
    "           1 .Yes, with subscription to an Internet service\n",
    "           2 .Yes, without a subscription to an Internet service\n",
    "           3 .No Internet access at this house, apartment, or mobile home\n",
    "  - `BATH`\n",
    "  - `RWAT`\n",
    "  - `PLM`\n",
    "  - `TEL`\n",
    "  - `TOIL`\n",
    "- A series of cost-related variables:\n",
    "  - `ELEP`:\n",
    "  - `FULP`:\n",
    "  - `GASP`:\n",
    "  - `WATP`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploring property value by state using ggplot maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a map of the United States with the states colored according to `VALP`, we can use ggplot2's `map_data` function. This functionality requires the [R package *maps*](https://cran.r-project.org/web/packages/maps/index.html), that you might need to install from the R shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you might need to install maps by running from the R console:\n",
    "# install.packages(\"maps\")\n",
    "\n",
    "library(ggplot2)\n",
    "\n",
    "states_map <- map_data(\"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our map contains a series of geometric shapes, latitudes, codes, and region names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t15537 obs. of  6 variables:\n",
      " $ long     : num  -87.5 -87.5 -87.5 -87.5 -87.6 ...\n",
      " $ lat      : num  30.4 30.4 30.4 30.3 30.3 ...\n",
      " $ group    : num  1 1 1 1 1 1 1 1 1 1 ...\n",
      " $ order    : int  1 2 3 4 5 6 7 8 9 10 ...\n",
      " $ region   : chr  \"alabama\" \"alabama\" \"alabama\" \"alabama\" ...\n",
      " $ subregion: chr  NA NA NA NA ...\n"
     ]
    }
   ],
   "source": [
    "str(states_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `region` variable there is the state name in the case of the *state* map, corresponding to the US states. We will need to match that with our housing dataset state code some way. But let's first reduce the housing dataset to what we want to represent in the map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we want to visualise is the average property value by state. We can use SparkR as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_avg_valp <- collect(\n",
    "    agg(\n",
    "        groupBy(housing_df, \"ST\"),\n",
    "        AVG_VALP=avg(housing_df$VALP)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>ST</th><th scope=col>AVG_VALP</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>31</td><td>141933.6</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>32</td><td>208622</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>33</td><td>267999</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>34</td><td>372496.1</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>35</td><td>194513.9</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>36</td><td>360343.5</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & ST & AVG_VALP\\\\\n",
       "\\hline\n",
       "\t1 & 31 & 141933.6\\\\\n",
       "\t2 & 32 & 208622\\\\\n",
       "\t3 & 33 & 267999\\\\\n",
       "\t4 & 34 & 372496.1\\\\\n",
       "\t5 & 35 & 194513.9\\\\\n",
       "\t6 & 36 & 360343.5\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  ST AVG_VALP\n",
       "1 31 141933.6\n",
       "2 32 208622.0\n",
       "3 33 267999.0\n",
       "4 34 372496.1\n",
       "5 35 194513.9\n",
       "6 36 360343.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(housing_avg_valp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we are ready to merge the dataset with the map as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_data <- merge(states_map, housing_avg_valp, by=\"REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in merge(statesMap, Dataset, by = \"region\"): error in evaluating the argument 'x' in selecting a method for function 'merge': Error: object 'statesMap' not found\n\n",
     "output_type": "error",
     "traceback": [
      "Error in merge(statesMap, Dataset, by = \"region\"): error in evaluating the argument 'x' in selecting a method for function 'merge': Error: object 'statesMap' not found\n\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in ggplot(MergedData, aes(x = long, y = lat, group = group, fill = Variable1)): object 'MergedData' not found\n",
     "output_type": "error",
     "traceback": [
      "Error in ggplot(MergedData, aes(x = long, y = lat, group = group, fill = Variable1)): object 'MergedData' not found\n"
     ]
    }
   ],
   "source": [
    "ggplot(MergedData, aes(x = long, y = lat, group = group, fill = Variable1)) + geom_polygon(color = \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
