{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models with SparkR: uses and present limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Introduction to Apache Spark with R by J. A. Dianes**](https://github.com/jadianes/spark-r-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use SparkR machine learning capabilities in order to predict property value in relation to other variables in the [2013 American Community Survey](http://www.census.gov/programs-surveys/acs/data/summary-file.html) dataset. The whole point of R on Spark is to introduce Spark scalability into R data analysis pipelines. With this idea in mind, we have seen how [SparkR documentation](http://spark.apache.org/docs/latest/sparkr.html) introduces data types and functions that are very similar to what we are used to when using regular R. The nest step in our series of notebooks will deal with its machine learning capabilities. While building a linear model we will also check the significance of each of the variables involved in building such a predictor for property value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a SparkSQL context and loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to explore our data, we first need to load it into a SparkSQL data frame. But first we need to init a SparkSQL context. The first thing we need to do is to set up some environment variables and library paths as follows. Remember to replace the value assigned to `SPARK_HOME` with your Spark home folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Spark home and R libs\n",
    "Sys.setenv(SPARK_HOME='/home/cluster/spark-1.5.0-bin-hadoop2.6')\n",
    ".libPaths(c(file.path(Sys.getenv('SPARK_HOME'), 'R', 'lib'), .libPaths()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the `SparkR` library as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘SparkR’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, na.omit\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, rbind, sample, subset, summary, table, transform\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(SparkR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can initialise the Spark context as [in the official documentation](http://spark.apache.org/docs/latest/sparkr.html#starting-up-sparkcontext-sqlcontext). In our case we are use a standalone Spark cluster with one master and seven workers. If you are running Spark in local node, use just `master='local'`. Additionally, we require a Spark package from Databricks to read CSV files (more on this in the [previous notebook](https://github.com/jadianes/spark-r-notebooks/blob/master/notebooks/nb1-spark-sql-basics/nb1-spark-sql-basics.ipynb)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching java with spark-submit command /home/cluster/spark-1.5.0-bin-hadoop2.6/bin/spark-submit  --packages com.databricks:spark-csv_2.11:1.2.0 sparkr-shell /tmp/RtmpmF2Hmf/backend_port6c9817062e87 \n"
     ]
    }
   ],
   "source": [
    "sc <- sparkR.init(master='spark://169.254.206.2:7077', sparkPackages=\"com.databricks:spark-csv_2.11:1.2.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can start the SparkSQL context as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext <- sparkRSQL.init(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our SparkSQL context ready, we can use it to load our CSV data into data frames. We have downloaded our [2013 American Community Survey dataset](http://www.census.gov/programs-surveys/acs/data/summary-file.html) files in [notebook 0](https://github.com/jadianes/spark-r-notebooks/tree/master/notebooks/nb0-starting-up/nb0-starting-up.ipynb), so they should be stored locally. Remember to set the right path for your data files in the first line, ours is `/nfs/data/2013-acs/ss13husa.csv`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_file_path <- file.path('', 'nfs','data','2013-acs','ss13husa.csv')\n",
    "housing_b_file_path <- file.path('', 'nfs','data','2013-acs','ss13husb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read into a SparkSQL dataframe. We need to pass four parameters in addition to the `sqlContext`:  \n",
    "\n",
    "- The file path.  \n",
    "- `header='true'` since our `csv` files have a header with the column names. \n",
    "- Indicate that we want the library to infer the schema.  \n",
    "- And the source type (the Databricks package in this case). \n",
    "\n",
    "And we have two separate files for both, housing and population data. We need to join them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_a_df <- read.df(sqlContext, \n",
    "                        housing_a_file_path, \n",
    "                        header='true', \n",
    "                        source = \"com.databricks.spark.csv\", \n",
    "                        inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_b_df <- read.df(sqlContext, \n",
    "                        housing_b_file_path, \n",
    "                        header='true', \n",
    "                        source = \"com.databricks.spark.csv\", \n",
    "                        inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_df <- rbind(housing_a_df, housing_b_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have everything there by counting the files and listing a few of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1476313"
      ],
      "text/latex": [
       "1476313"
      ],
      "text/markdown": [
       "1476313"
      ],
      "text/plain": [
       "[1] 1476313"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows <- nrow(housing_df)\n",
    "nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>RT</th><th scope=col>SERIALNO</th><th scope=col>DIVISION</th><th scope=col>PUMA</th><th scope=col>REGION</th><th scope=col>ST</th><th scope=col>ADJHSG</th><th scope=col>ADJINC</th><th scope=col>WGTP</th><th scope=col>NP</th><th scope=col>ellip.h</th><th scope=col>wgtp71</th><th scope=col>wgtp72</th><th scope=col>wgtp73</th><th scope=col>wgtp74</th><th scope=col>wgtp75</th><th scope=col>wgtp76</th><th scope=col>wgtp77</th><th scope=col>wgtp78</th><th scope=col>wgtp79</th><th scope=col>wgtp80</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>H</td><td>84</td><td>6</td><td>2600</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>0</td><td>1</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>H</td><td>154</td><td>6</td><td>2500</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>51</td><td>4</td><td>⋯</td><td>86</td><td>53</td><td>59</td><td>84</td><td>49</td><td>15</td><td>15</td><td>20</td><td>50</td><td>16</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>H</td><td>156</td><td>6</td><td>1700</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>449</td><td>1</td><td>⋯</td><td>161</td><td>530</td><td>601</td><td>579</td><td>341</td><td>378</td><td>387</td><td>421</td><td>621</td><td>486</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>H</td><td>160</td><td>6</td><td>2200</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>16</td><td>3</td><td>⋯</td><td>31</td><td>24</td><td>33</td><td>7</td><td>7</td><td>13</td><td>18</td><td>23</td><td>23</td><td>5</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>H</td><td>231</td><td>6</td><td>2400</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>52</td><td>1</td><td>⋯</td><td>21</td><td>18</td><td>37</td><td>49</td><td>103</td><td>38</td><td>49</td><td>51</td><td>46</td><td>47</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>H</td><td>286</td><td>6</td><td>900</td><td>3</td><td>1</td><td>1000000</td><td>1007549</td><td>76</td><td>1</td><td>⋯</td><td>128</td><td>25</td><td>68</td><td>66</td><td>80</td><td>26</td><td>66</td><td>164</td><td>88</td><td>24</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll}\n",
       "  & RT & SERIALNO & DIVISION & PUMA & REGION & ST & ADJHSG & ADJINC & WGTP & NP & ellip.h & wgtp71 & wgtp72 & wgtp73 & wgtp74 & wgtp75 & wgtp76 & wgtp77 & wgtp78 & wgtp79 & wgtp80\\\\\n",
       "\\hline\n",
       "\t1 & H & 84 & 6 & 2600 & 3 & 1 & 1000000 & 1007549 & 0 & 1 & ⋯ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
       "\t2 & H & 154 & 6 & 2500 & 3 & 1 & 1000000 & 1007549 & 51 & 4 & ⋯ & 86 & 53 & 59 & 84 & 49 & 15 & 15 & 20 & 50 & 16\\\\\n",
       "\t3 & H & 156 & 6 & 1700 & 3 & 1 & 1000000 & 1007549 & 449 & 1 & ⋯ & 161 & 530 & 601 & 579 & 341 & 378 & 387 & 421 & 621 & 486\\\\\n",
       "\t4 & H & 160 & 6 & 2200 & 3 & 1 & 1000000 & 1007549 & 16 & 3 & ⋯ & 31 & 24 & 33 & 7 & 7 & 13 & 18 & 23 & 23 & 5\\\\\n",
       "\t5 & H & 231 & 6 & 2400 & 3 & 1 & 1000000 & 1007549 & 52 & 1 & ⋯ & 21 & 18 & 37 & 49 & 103 & 38 & 49 & 51 & 46 & 47\\\\\n",
       "\t6 & H & 286 & 6 & 900 & 3 & 1 & 1000000 & 1007549 & 76 & 1 & ⋯ & 128 & 25 & 68 & 66 & 80 & 26 & 66 & 164 & 88 & 24\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "  RT SERIALNO DIVISION PUMA REGION ST  ADJHSG  ADJINC WGTP NP TYPE ACCESS ACR\n",
       "1  H       84        6 2600      3  1 1000000 1007549    0  1    3     NA  NA\n",
       "2  H      154        6 2500      3  1 1000000 1007549   51  4    1      1   1\n",
       "3  H      156        6 1700      3  1 1000000 1007549  449  1    1      3   1\n",
       "4  H      160        6 2200      3  1 1000000 1007549   16  3    1      3   3\n",
       "5  H      231        6 2400      3  1 1000000 1007549   52  1    1      3  NA\n",
       "6  H      286        6  900      3  1 1000000 1007549   76  1    1      1   1\n",
       "  AGS BATH BDSP BLD BROADBND BUS COMPOTHX CONP DIALUP DSL ELEP FIBEROP FS FULP\n",
       "1  NA   NA   NA  NA       NA  NA       NA   NA     NA  NA   NA      NA  2   NA\n",
       "2  NA    1    3   2        2   2        2   NA      2   2  350       1  2    2\n",
       "3  NA    1    3   2       NA   2        2   NA     NA  NA  300      NA  2    2\n",
       "4   1    1    4   2       NA   2        2   NA     NA  NA  220      NA  1    2\n",
       "5  NA    1    1   5       NA  NA        2   NA     NA  NA   60      NA  1    1\n",
       "6  NA    1    4   2        2   2        2   NA      2   2  100       2  2    2\n",
       "  GASP HANDHELD HFL INSP LAPTOP MHP MODEM MRGI MRGP MRGT MRGX OTHSVCEX REFR\n",
       "1   NA       NA  NA   NA     NA  NA    NA   NA   NA   NA   NA       NA   NA\n",
       "2    3        2   3  350      1  NA     2   NA   NA   NA    3        2    1\n",
       "3    3        2   3  980      1  NA    NA    1  550    2    1       NA    1\n",
       "4   20        1   3   NA      2  NA    NA   NA   NA   NA   NA       NA    1\n",
       "5    3        2   1   NA      2  NA    NA   NA   NA   NA   NA       NA    1\n",
       "6   90        2   1   50      2  NA     1    2  290    2    1        2    1\n",
       "  RMSP RNTM RNTP RWAT RWATPR SATELLITE SINK SMP STOV TEL TEN TOIL VACS  VALP\n",
       "1   NA   NA   NA   NA     NA        NA   NA  NA   NA  NA  NA   NA   NA    NA\n",
       "2    9   NA   NA    1      9         2    1  NA    1   1   2    1   NA 25000\n",
       "3    6   NA   NA    1      9        NA    1  NA    1   1   1    1   NA 80000\n",
       "4    6    2  100    1      9        NA    1  NA    1   1   3    1   NA    NA\n",
       "5    3    2   80    1      9        NA    1  NA    1   1   3    1   NA    NA\n",
       "6    9   NA   NA    1      9         2    1  NA    1   1   1    1   NA 18000\n",
       "  VEH WATP YBL FES FFINCP FGRNTP FHINCP  FINCP FPARC FSMOCP GRNTP GRPIP HHL HHT\n",
       "1  NA   NA  NA  NA     NA     NA     NA     NA    NA     NA    NA    NA  NA  NA\n",
       "2   3  480   2   1      1     NA      1 151000     4      0    NA    NA   1   1\n",
       "3   1  700   5  NA      0     NA      1     NA    NA      1    NA    NA   1   6\n",
       "4   2  360   6   8      1      0      1  11400     2     NA   370    39   1   3\n",
       "5   0    1   5  NA      0      0      0     NA    NA     NA   140    43   1   6\n",
       "6   1  370   2  NA      0     NA      1     NA    NA      1    NA    NA   1   6\n",
       "   HINCP HUGCL HUPAC HUPAOC HUPARC KIT LNGI MULTG MV NOC NPF NPP NR NRC OCPIP\n",
       "1     NA    NA    NA     NA     NA  NA   NA    NA NA  NA  NA  NA NA  NA    NA\n",
       "2 151000     0     4      4      4   1    1     1  6   0   4   0  0   0     3\n",
       "3  39930     0     4      4      4   1    1     1  7   0  NA   0  0   0    28\n",
       "4  11400     1     2      4      2   1    1     1  4   0   3   1  0   1    NA\n",
       "5   3900     0     4      4      4   1    1     1  6   0  NA   0  0   0    NA\n",
       "6   5400     0     4      4      4   1    1     1  7   0  NA   0  0   0   101\n",
       "  PARTNER PLM PSF R18 R60 R65 RESMODE SMOCP SMX SRNT SSMC SVAL TAXP WIF WKEXREL\n",
       "1      NA  NA  NA  NA  NA  NA      NA    NA  NA   NA   NA   NA   NA  NA      NA\n",
       "2       0   1   0   0   0   0       2   426  NA    0    0    1    3   2       1\n",
       "3       0   1   0   0   1   0       2   926   3    0    0    1    6  NA      NA\n",
       "4       0   1   0   1   1   0       2    NA  NA    0    0    0   NA   1      15\n",
       "5       0   1   0   0   1   1       1    NA  NA    1    0    0   NA  NA      NA\n",
       "6       0   1   0   0   1   1       1   522   3    0    0    1    3  NA      NA\n",
       "  WORKSTAT FACCESSP FACRP FAGSP FBATHP FBDSP FBLDP FBROADBNDP FBUSP FCOMPOTHXP\n",
       "1       NA       NA    NA    NA     NA    NA    NA         NA    NA         NA\n",
       "2        1        0     0     0      0     0     0          0     0          0\n",
       "3       NA        0     0     0      0     0     0          0     0          0\n",
       "4       15        0     1     1      0     0     0          0     0          0\n",
       "5       NA        0     0     0      0     0     0          0     0          0\n",
       "6       NA        0     1     0      0     0     1          1     1          1\n",
       "  FCONP FDIALUPP FDSLP FELEP FFIBEROPP FFSP FFULP FGASP FHANDHELDP FHFLP FINSP\n",
       "1    NA       NA    NA    NA        NA    0    NA    NA         NA    NA    NA\n",
       "2     0        0     0     0         0    0     0     0          0     0     1\n",
       "3     0        0     0     0         0    0     0     0          0     0     1\n",
       "4     0        0     0     0         0    0     0     0          0     0     0\n",
       "5     0        0     0     0         0    0     0     0          0     0     0\n",
       "6     0        1     1     1         1    0     1     1          1     0     1\n",
       "  FKITP FLAPTOPP FMHP FMODEMP FMRGIP FMRGP FMRGTP FMRGXP FMVP FOTHSVCEXP FPLMP\n",
       "1    NA       NA   NA      NA     NA    NA     NA     NA    0         NA    NA\n",
       "2     0        0    0       0      0     0      0      0    0          0     0\n",
       "3     0        0    0       0      1     1      1      1    1          0     0\n",
       "4     0        0    0       0      0     0      0      0    0          0     0\n",
       "5     0        0    0       0      0     0      0      0    0          0     0\n",
       "6     0        1    0       1      1     1      1      1    0          1     0\n",
       "  FREFRP FRMSP FRNTMP FRNTP FRWATP FRWATPRP FSATELLITEP FSINKP FSMP FSMXHP\n",
       "1     NA    NA     NA    NA     NA       NA          NA     NA   NA     NA\n",
       "2      0     0      0     0      0        0           0      0    0      0\n",
       "3      0     0      0     0      0        0           0      0    1      1\n",
       "4      0     0      0     0      0        0           0      0    0      0\n",
       "5      0     0      0     0      0        0           0      0    0      0\n",
       "6      0     0      0     0      0        0           1      0    1      1\n",
       "  FSMXSP FSTOVP FTAXP FTELP FTENP FTOILP FVACSP FVALP FVEHP FWATP FYBLP wgtp1\n",
       "1     NA     NA    NA    NA    NA     NA     NA    NA    NA    NA    NA     0\n",
       "2      0      0     1     0     0      0      0     0     0     0     0    45\n",
       "3      1      0     1     0     1      0      0     1     0     0     0   481\n",
       "4      0      0     0     0     0      0      0     0     0     0     0    21\n",
       "5      0      0     0     0     0      0      0     0     0     0     0    74\n",
       "6      1      0     1     0     1      0      0     1     0     1     0    84\n",
       "  wgtp2 wgtp3 wgtp4 wgtp5 wgtp6 wgtp7 wgtp8 wgtp9 wgtp10 wgtp11 wgtp12 wgtp13\n",
       "1     0     0     0     0     0     0     0     0      0      0      0      0\n",
       "2    52    53    50   100    79    78    50    19     57     89     46     67\n",
       "3   576   807   739   154   173   722   455   157    671    681    470    403\n",
       "4    14     6    20    30    16     5    25     4     27     11     28     14\n",
       "5    50    43    13    12   102    80    20    60     98     17     18     52\n",
       "6    69    19   118    91    77    20    76    73    180     68     78     24\n",
       "  wgtp14 wgtp15 wgtp16 wgtp17 wgtp18 wgtp19 wgtp20 wgtp21 wgtp22 wgtp23 wgtp24\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2    109     51     18     17     18     47     87     49     50     49     64\n",
       "3    118    339    436    443    393    160    411    468    387    127    150\n",
       "4     15     27     40     31      4     16     20      9     26     39     32\n",
       "5     45     95     57     46     75     47     54     16     43     57    122\n",
       "6    131    136     73    143     82     26     67     21     27     63     74\n",
       "  wgtp25 wgtp26 wgtp27 wgtp28 wgtp29 wgtp30 wgtp31 wgtp32 wgtp33 wgtp34 wgtp35\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     13     16     16     55     86     48     14     49     50     11     56\n",
       "3    553    514    133    613    562    151    196    432    577    710    378\n",
       "4     38      9     15      6     19     22      5     16      3     27     21\n",
       "5     93     17     13     61     59     23     82     69     50     55     20\n",
       "6     24    119     87    126    131     67    130     30     79     81     70\n",
       "  wgtp36 wgtp37 wgtp38 wgtp39 wgtp40 wgtp41 wgtp42 wgtp43 wgtp44 wgtp45 wgtp46\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     83     71    108     54     81     43     53     50     63     17     15\n",
       "3    481    504    531    760    409    560    404    637    516    101    150\n",
       "4     33     14     11      6     31     10     21     22     11      8     22\n",
       "5     59     62     44     38     40     16     47     40    107     88     16\n",
       "6     19     94    119     65     23     84     83     23    111     70     73\n",
       "  wgtp47 wgtp48 wgtp49 wgtp50 wgtp51 wgtp52 wgtp53 wgtp54 wgtp55 wgtp56 wgtp57\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     15     47     88     47     14     52     51     20     53     84     78\n",
       "3    659    454    179    793    724    451    527    168    355    445    465\n",
       "4     27     16     30      6     28      9     17     12     23      3      4\n",
       "5     14     86     59     16     92    113     61     64     14     74     48\n",
       "6     22     86     69    123     91     75     28    138     98     69    121\n",
       "  wgtp58 wgtp59 wgtp60 wgtp61 wgtp62 wgtp63 wgtp64 wgtp65 wgtp66 wgtp67 wgtp68\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     74     53     15     54     55     58     65    110     82     89     50\n",
       "3    426    228    575    386    433    167    230    797    572    155    556\n",
       "4     38     15     17     33      8     10      7      7     21     22     19\n",
       "5     43     53     50     83     47     51     15     16     93     95     15\n",
       "6     87     23     67     18     24     77     62     28    125     72    121\n",
       "  wgtp69 wgtp70 wgtp71 wgtp72 wgtp73 wgtp74 wgtp75 wgtp76 wgtp77 wgtp78 wgtp79\n",
       "1      0      0      0      0      0      0      0      0      0      0      0\n",
       "2     12     55     86     53     59     84     49     15     15     20     50\n",
       "3    596    170    161    530    601    579    341    378    387    421    621\n",
       "4     12     11     31     24     33      7      7     13     18     23     23\n",
       "5     47    106     21     18     37     49    103     38     49     51     46\n",
       "6    135    100    128     25     68     66     80     26     66    164     88\n",
       "  wgtp80\n",
       "1      0\n",
       "2     16\n",
       "3    486\n",
       "4      5\n",
       "5     47\n",
       "6     24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head(housing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert `ST` (or any other categorical variable) from a numeric variable into a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_df$ST <- cast(housing_df$ST, \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_df$REGION <- cast(housing_df$REGION, \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we need either to impute values or to remove samples with null values in any of our predictors or desponse. For the response (`VALP`) we will use just those samples with actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_with_valp_df <- filter(\n",
    "    housing_df, \n",
    "    isNotNull(housing_df$VALP) \n",
    "    & isNotNull(housing_df$TAXP)\n",
    "    & isNotNull(housing_df$INSP)\n",
    "    & isNotNull(housing_df$ACR)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the remaining samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "807202"
      ],
      "text/latex": [
       "807202"
      ],
      "text/markdown": [
       "807202"
      ],
      "text/plain": [
       "[1] 807202"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows <- nrow(housing_with_valp_df)\n",
    "nrows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a train / test data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a split function in SparkR, but we can use `sample` in combination with the `SERIALNO` column in order to prepare two sets of IDs for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "housing_df_test <- sample(housing_with_valp_df,FALSE,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "80511"
      ],
      "text/latex": [
       "80511"
      ],
      "text/markdown": [
       "80511"
      ],
      "text/plain": [
       "[1] 80511"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrow(housing_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_ids <- collect(select(housing_df_test, \"SERIALNO\"))$SERIALNO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately SparkR doesn't support negative %in% expressions, so we need to do this in two steps. First we add a flag to the whole dataset indicating that a sample belongs to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_with_valp_df$IS_TEST <- housing_with_valp_df$SERIALNO %in% test_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we use that flag to subset out the train dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "housing_df_train <- subset(housing_with_valp_df, housing_with_valp_df$IS_TEST==FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "726691"
      ],
      "text/latex": [
       "726691"
      ],
      "text/markdown": [
       "726691"
      ],
      "text/plain": [
       "[1] 726691"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrow(housing_df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this approach is not very scalable since we are collecting all the test IDs and passing them over to build the new flag column. What if we have a much larger test set? Hopefully futre versions of SparkR will come up with a proper `split` functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a linear model, we call `glm` with the following parameters:\n",
    "- A formula: sadly, `SparkR::glm()` gives us an error when we pass more than eight variables using `+` in the formula.\n",
    "- The dataset we want to use to train the model.\n",
    "- The type of model (gaussian or binomial).  \n",
    "\n",
    "This doesn't differ much from the usual R `glm` command, although right now is more limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of variables we have used includes:  \n",
    "\n",
    "- `RMSP` or number of rooms.\n",
    "- `ACR` the lot size.\n",
    "- `INSP` or insurance cost.\n",
    "- `TAXP` or taxes cost.\n",
    "- `ELEP` or electricity cost.\n",
    "- `GASP` or gas cost.\n",
    "- `ST` that is the state code.\n",
    "- `REGION` that identifies the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model <- glm(\n",
    "    VALP ~ RMSP + ACR + INSP + TAXP + ELEP + GASP + ST + REGION, \n",
    "    data = housing_df_train, \n",
    "    family = \"gaussian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>$coefficients</strong> = <table>\n",
       "<thead><tr><th></th><th scope=col>Estimate</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-76528.93</td></tr>\n",
       "\t<tr><th scope=row>RMSP</th><td>14429.67</td></tr>\n",
       "\t<tr><th scope=row>ACR</th><td>31462.28</td></tr>\n",
       "\t<tr><th scope=row>INSP</th><td>99.35929</td></tr>\n",
       "\t<tr><th scope=row>TAXP</th><td>5625.895</td></tr>\n",
       "\t<tr><th scope=row>ELEP</th><td>194.3572</td></tr>\n",
       "\t<tr><th scope=row>GASP</th><td>233.6086</td></tr>\n",
       "\t<tr><th scope=row>ST__6</th><td>83708.07</td></tr>\n",
       "\t<tr><th scope=row>ST__48</th><td>-422438.3</td></tr>\n",
       "\t<tr><th scope=row>ST__12</th><td>-373593.9</td></tr>\n",
       "\t<tr><th scope=row>ST__36</th><td>-162774.3</td></tr>\n",
       "\t<tr><th scope=row>ST__42</th><td>-187752.5</td></tr>\n",
       "\t<tr><th scope=row>ST__39</th><td>-92930.34</td></tr>\n",
       "\t<tr><th scope=row>ST__17</th><td>-104259.1</td></tr>\n",
       "\t<tr><th scope=row>ST__26</th><td>-94614.73</td></tr>\n",
       "\t<tr><th scope=row>ST__37</th><td>-331767.1</td></tr>\n",
       "\t<tr><th scope=row>ST__13</th><td>-361383.3</td></tr>\n",
       "\t<tr><th scope=row>ST__51</th><td>-272701</td></tr>\n",
       "\t<tr><th scope=row>ST__34</th><td>-174525.3</td></tr>\n",
       "\t<tr><th scope=row>ST__18</th><td>-41841.98</td></tr>\n",
       "\t<tr><th scope=row>ST__47</th><td>-337911.6</td></tr>\n",
       "\t<tr><th scope=row>ST__53</th><td>-80898.74</td></tr>\n",
       "\t<tr><th scope=row>ST__55</th><td>-104367.1</td></tr>\n",
       "\t<tr><th scope=row>ST__29</th><td>-66378.45</td></tr>\n",
       "\t<tr><th scope=row>ST__27</th><td>-79352.28</td></tr>\n",
       "\t<tr><th scope=row>ST__4</th><td>-63673.31</td></tr>\n",
       "\t<tr><th scope=row>ST__24</th><td>-294930.5</td></tr>\n",
       "\t<tr><th scope=row>ST__25</th><td>-96789.48</td></tr>\n",
       "\t<tr><th scope=row>ST__1</th><td>-314002.7</td></tr>\n",
       "\t<tr><th scope=row>ST__8</th><td>-60126.29</td></tr>\n",
       "\t<tr><th scope=row>ST__45</th><td>-314902</td></tr>\n",
       "\t<tr><th scope=row>ST__21</th><td>-348317.8</td></tr>\n",
       "\t<tr><th scope=row>ST__22</th><td>-330038.1</td></tr>\n",
       "\t<tr><th scope=row>ST__41</th><td>-99094.19</td></tr>\n",
       "\t<tr><th scope=row>ST__40</th><td>-374563.1</td></tr>\n",
       "\t<tr><th scope=row>ST__19</th><td>-82441.49</td></tr>\n",
       "\t<tr><th scope=row>ST__9</th><td>-140932.9</td></tr>\n",
       "\t<tr><th scope=row>ST__5</th><td>-337026.4</td></tr>\n",
       "\t<tr><th scope=row>ST__20</th><td>-111871.6</td></tr>\n",
       "\t<tr><th scope=row>ST__28</th><td>-352687.7</td></tr>\n",
       "\t<tr><th scope=row>ST__49</th><td>-75785.26</td></tr>\n",
       "\t<tr><th scope=row>ST__32</th><td>-90557.16</td></tr>\n",
       "\t<tr><th scope=row>ST__54</th><td>-310321.3</td></tr>\n",
       "\t<tr><th scope=row>ST__35</th><td>-65105.3</td></tr>\n",
       "\t<tr><th scope=row>ST__31</th><td>-124406.9</td></tr>\n",
       "\t<tr><th scope=row>ST__16</th><td>-95069.85</td></tr>\n",
       "\t<tr><th scope=row>ST__23</th><td>-145797.1</td></tr>\n",
       "\t<tr><th scope=row>ST__33</th><td>-216322.5</td></tr>\n",
       "\t<tr><th scope=row>ST__30</th><td>-99628.01</td></tr>\n",
       "\t<tr><th scope=row>ST__10</th><td>-244316.6</td></tr>\n",
       "\t<tr><th scope=row>ST__44</th><td>-183127.4</td></tr>\n",
       "\t<tr><th scope=row>ST__15</th><td>264858.9</td></tr>\n",
       "\t<tr><th scope=row>ST__46</th><td>-69061.88</td></tr>\n",
       "\t<tr><th scope=row>ST__38</th><td>-63042.23</td></tr>\n",
       "\t<tr><th scope=row>ST__50</th><td>-208646.4</td></tr>\n",
       "\t<tr><th scope=row>ST__56</th><td>-103393.6</td></tr>\n",
       "\t<tr><th scope=row>ST__2</th><td>-65145.47</td></tr>\n",
       "\t<tr><th scope=row>REGION__3</th><td>202606.9</td></tr>\n",
       "\t<tr><th scope=row>REGION__2</th><td>-113094.2</td></tr>\n",
       "\t<tr><th scope=row>REGION__4</th><td>-3672.895</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\textbf{\\$coefficients} = \\begin{tabular}{r|l}\n",
       "  & Estimate\\\\\n",
       "\\hline\n",
       "\t(Intercept) & -76528.93\\\\\n",
       "\tRMSP & 14429.67\\\\\n",
       "\tACR & 31462.28\\\\\n",
       "\tINSP & 99.35929\\\\\n",
       "\tTAXP & 5625.895\\\\\n",
       "\tELEP & 194.3572\\\\\n",
       "\tGASP & 233.6086\\\\\n",
       "\tST__6 & 83708.07\\\\\n",
       "\tST__48 & -422438.3\\\\\n",
       "\tST__12 & -373593.9\\\\\n",
       "\tST__36 & -162774.3\\\\\n",
       "\tST__42 & -187752.5\\\\\n",
       "\tST__39 & -92930.34\\\\\n",
       "\tST__17 & -104259.1\\\\\n",
       "\tST__26 & -94614.73\\\\\n",
       "\tST__37 & -331767.1\\\\\n",
       "\tST__13 & -361383.3\\\\\n",
       "\tST__51 & -272701\\\\\n",
       "\tST__34 & -174525.3\\\\\n",
       "\tST__18 & -41841.98\\\\\n",
       "\tST__47 & -337911.6\\\\\n",
       "\tST__53 & -80898.74\\\\\n",
       "\tST__55 & -104367.1\\\\\n",
       "\tST__29 & -66378.45\\\\\n",
       "\tST__27 & -79352.28\\\\\n",
       "\tST__4 & -63673.31\\\\\n",
       "\tST__24 & -294930.5\\\\\n",
       "\tST__25 & -96789.48\\\\\n",
       "\tST__1 & -314002.7\\\\\n",
       "\tST__8 & -60126.29\\\\\n",
       "\tST__45 & -314902\\\\\n",
       "\tST__21 & -348317.8\\\\\n",
       "\tST__22 & -330038.1\\\\\n",
       "\tST__41 & -99094.19\\\\\n",
       "\tST__40 & -374563.1\\\\\n",
       "\tST__19 & -82441.49\\\\\n",
       "\tST__9 & -140932.9\\\\\n",
       "\tST__5 & -337026.4\\\\\n",
       "\tST__20 & -111871.6\\\\\n",
       "\tST__28 & -352687.7\\\\\n",
       "\tST__49 & -75785.26\\\\\n",
       "\tST__32 & -90557.16\\\\\n",
       "\tST__54 & -310321.3\\\\\n",
       "\tST__35 & -65105.3\\\\\n",
       "\tST__31 & -124406.9\\\\\n",
       "\tST__16 & -95069.85\\\\\n",
       "\tST__23 & -145797.1\\\\\n",
       "\tST__33 & -216322.5\\\\\n",
       "\tST__30 & -99628.01\\\\\n",
       "\tST__10 & -244316.6\\\\\n",
       "\tST__44 & -183127.4\\\\\n",
       "\tST__15 & 264858.9\\\\\n",
       "\tST__46 & -69061.88\\\\\n",
       "\tST__38 & -63042.23\\\\\n",
       "\tST__50 & -208646.4\\\\\n",
       "\tST__56 & -103393.6\\\\\n",
       "\tST__2 & -65145.47\\\\\n",
       "\tREGION__3 & 202606.9\\\\\n",
       "\tREGION__2 & -113094.2\\\\\n",
       "\tREGION__4 & -3672.895\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "**$coefficients** = 1. -76528.9343642711\n",
       "2. 14429.6676413534\n",
       "3. 31462.2807895603\n",
       "4. 99.3592935017777\n",
       "5. 5625.89534979718\n",
       "6. 194.357184476621\n",
       "7. 233.608594567802\n",
       "8. 83708.0734879215\n",
       "9. -422438.290298926\n",
       "10. -373593.917266375\n",
       "11. -162774.327340436\n",
       "12. -187752.546958428\n",
       "13. -92930.3360086799\n",
       "14. -104259.085694771\n",
       "15. -94614.7343026621\n",
       "16. -331767.130223792\n",
       "17. -361383.260518881\n",
       "18. -272700.965425446\n",
       "19. -174525.271081924\n",
       "20. -41841.9839949333\n",
       "21. -337911.598078607\n",
       "22. -80898.7415354283\n",
       "23. -104367.12002188\n",
       "24. -66378.4516493276\n",
       "25. -79352.275470081\n",
       "26. -63673.3061408653\n",
       "27. -294930.506265753\n",
       "28. -96789.4802758863\n",
       "29. -314002.679884603\n",
       "30. -60126.2878629437\n",
       "31. -314901.990639022\n",
       "32. -348317.832609482\n",
       "33. -330038.124978577\n",
       "34. -99094.1915821918\n",
       "35. -374563.065875535\n",
       "36. -82441.4869155404\n",
       "37. -140932.880916578\n",
       "38. -337026.433426152\n",
       "39. -111871.587532204\n",
       "40. -352687.680689602\n",
       "41. -75785.2574112619\n",
       "42. -90557.1568419188\n",
       "43. -310321.26379843\n",
       "44. -65105.2979743312\n",
       "45. -124406.900344044\n",
       "46. -95069.8467176206\n",
       "47. -145797.096078935\n",
       "48. -216322.49983083\n",
       "49. -99628.0072332824\n",
       "50. -244316.555925714\n",
       "51. -183127.442533971\n",
       "52. 264858.915706793\n",
       "53. -69061.8806141694\n",
       "54. -63042.2288091552\n",
       "55. -208646.376216023\n",
       "56. -103393.592071623\n",
       "57. -65145.474700118\n",
       "58. 202606.896638947\n",
       "59. -113094.232216353\n",
       "60. -3672.894568644\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$coefficients\n",
       "                 Estimate\n",
       "(Intercept)  -76528.93436\n",
       "RMSP          14429.66764\n",
       "ACR           31462.28079\n",
       "INSP             99.35929\n",
       "TAXP           5625.89535\n",
       "ELEP            194.35718\n",
       "GASP            233.60859\n",
       "ST__6         83708.07349\n",
       "ST__48      -422438.29030\n",
       "ST__12      -373593.91727\n",
       "ST__36      -162774.32734\n",
       "ST__42      -187752.54696\n",
       "ST__39       -92930.33601\n",
       "ST__17      -104259.08569\n",
       "ST__26       -94614.73430\n",
       "ST__37      -331767.13022\n",
       "ST__13      -361383.26052\n",
       "ST__51      -272700.96543\n",
       "ST__34      -174525.27108\n",
       "ST__18       -41841.98399\n",
       "ST__47      -337911.59808\n",
       "ST__53       -80898.74154\n",
       "ST__55      -104367.12002\n",
       "ST__29       -66378.45165\n",
       "ST__27       -79352.27547\n",
       "ST__4        -63673.30614\n",
       "ST__24      -294930.50627\n",
       "ST__25       -96789.48028\n",
       "ST__1       -314002.67988\n",
       "ST__8        -60126.28786\n",
       "ST__45      -314901.99064\n",
       "ST__21      -348317.83261\n",
       "ST__22      -330038.12498\n",
       "ST__41       -99094.19158\n",
       "ST__40      -374563.06588\n",
       "ST__19       -82441.48692\n",
       "ST__9       -140932.88092\n",
       "ST__5       -337026.43343\n",
       "ST__20      -111871.58753\n",
       "ST__28      -352687.68069\n",
       "ST__49       -75785.25741\n",
       "ST__32       -90557.15684\n",
       "ST__54      -310321.26380\n",
       "ST__35       -65105.29797\n",
       "ST__31      -124406.90034\n",
       "ST__16       -95069.84672\n",
       "ST__23      -145797.09608\n",
       "ST__33      -216322.49983\n",
       "ST__30       -99628.00723\n",
       "ST__10      -244316.55593\n",
       "ST__44      -183127.44253\n",
       "ST__15       264858.91571\n",
       "ST__46       -69061.88061\n",
       "ST__38       -63042.22881\n",
       "ST__50      -208646.37622\n",
       "ST__56      -103393.59207\n",
       "ST__2        -65145.47470\n",
       "REGION__3    202606.89664\n",
       "REGION__2   -113094.23222\n",
       "REGION__4     -3672.89457\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, signif.stars=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, the current version of `SparkR::summary()` doesn't provide significance starts. That makes model interpretation and selection very difficult. But at least we know how each variables influences a property value. For example, the Midwest region decreases property value, while the West increases it, etc. In order to interpret that we need to have a look at our [data dictionary](http://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict13.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, since we don't have significance starts, we can iterate through adding/removing variables and calculating the R2 value. In our case we ended up with the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model using the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's obtain the average value for `VALP` that we will use as a reference of a base predictor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALP_mean <- collect(agg(\n",
    "    housing_df_train, \n",
    "    AVG_VALP=mean(housing_df_train$VALP)\n",
    "))$AVG_VALP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "245616.538322341"
      ],
      "text/latex": [
       "245616.538322341"
      ],
      "text/markdown": [
       "245616.538322341"
      ],
      "text/plain": [
       "[1] 245616.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALP_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now predict on our test dataset as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions <- predict(model, newData = housing_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the squared residuals and squared totals so later on we can calculate [R2](https://en.wikipedia.org/wiki/Coefficient_of_determination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>VALP</th><th scope=col>prediction</th><th scope=col>S_res</th><th scope=col>S_tot</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>18000</td><td>35710.71</td><td>313669413</td><td>51809288518</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>60000</td><td>114905.2</td><td>3014584491</td><td>34453499299</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>750000</td><td>861633.8</td><td>12462110173</td><td>254402676414</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>300000</td><td>196937.2</td><td>10621941691</td><td>2957560904</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>40000</td><td>22703.85</td><td>299156942</td><td>42278160832</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>60000</td><td>121898.5</td><td>3831423241</td><td>34453499299</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & VALP & prediction & S_res & S_tot\\\\\n",
       "\\hline\n",
       "\t1 & 18000 & 35710.71 & 313669413 & 51809288518\\\\\n",
       "\t2 & 60000 & 114905.2 & 3014584491 & 34453499299\\\\\n",
       "\t3 & 750000 & 861633.8 & 12462110173 & 254402676414\\\\\n",
       "\t4 & 300000 & 196937.2 & 10621941691 & 2957560904\\\\\n",
       "\t5 & 40000 & 22703.85 & 299156942 & 42278160832\\\\\n",
       "\t6 & 60000 & 121898.5 & 3831423241 & 34453499299\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "    VALP prediction       S_res        S_tot\n",
       "1  18000   35710.71   313669413  51809288518\n",
       "2  60000  114905.23  3014584491  34453499299\n",
       "3 750000  861633.82 12462110173 254402676414\n",
       "4 300000  196937.20 10621941691   2957560904\n",
       "5  40000   22703.85   299156942  42278160832\n",
       "6  60000  121898.49  3831423241  34453499299"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions <- transform(\n",
    "    predictions, \n",
    "    S_res=(predictions$VALP - predictions$prediction)**2, \n",
    "    S_tot=(predictions$VALP - VALP_mean)**2)\n",
    "head(select(predictions, \"VALP\", \"prediction\", \"S_res\", \"S_tot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nrows_test <- nrow(housing_df_test)\n",
    "residuals <- collect(agg(\n",
    "    predictions, \n",
    "    SS_res=sum(predictions$S_res),\n",
    "    SS_tot=sum(predictions$S_tot)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>SS_res</th><th scope=col>SS_tot</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>5.108513e+15</td><td>8.635319e+15</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & SS_res & SS_tot\\\\\n",
       "\\hline\n",
       "\t1 & 5.108513e+15 & 8.635319e+15\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "        SS_res       SS_tot\n",
       "1 5.108513e+15 8.635319e+15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R2 <- 1.0 - (residuals$SS_res/residuals$SS_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.408416475680193"
      ],
      "text/latex": [
       "0.408416475680193"
      ],
      "text/markdown": [
       "0.408416475680193"
      ],
      "text/plain": [
       "[1] 0.4084165"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression, the R2 coefficient of determination is a statistical measure of how well the regression line approximates the real data points. An R2 of 1 indicates that the regression line perfectly fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 0.41 doesn't speak very well about our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to improve our model if we really want to be able to predict property values. However there are some limitations in the current SparkR implementation that stop us from doing so. Hopefully these limitations won't be there in further versions. Moreover, we are using a linear model, and the relationships between our predictors and the target variable might not be linear at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But right now, in Spark v1.5, the R machine learning capabilities are still very limited. We are missing a few things, such as: \n",
    "\n",
    "- Accepting more than 8 variables in formulas using `+`.  \n",
    "- Having significance stars that help model interpretation and selection.  \n",
    "- Having other indicators (e.g. R2) in summary objects so we don't have to calculate them ourselves.\n",
    "- Being able to create more complex formulas (e.g. removing intercepts using 100 + ...) so we don't get negative values, etc.\n",
    "- Although we have a `sample` method, we are missing a `split` one that we can use to easier have train/test splits.  \n",
    "- Being able to use more powerful models (or at least models that deal better with non linearities), and not just linear ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
